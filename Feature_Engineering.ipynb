{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846e1c83-2a6b-446f-9a83-0a4376a0efd5",
   "metadata": {
    "id": "846e1c83-2a6b-446f-9a83-0a4376a0efd5"
   },
   "source": [
    "# Feature Engineering ‚öôÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t9YkGI2J1LuY",
   "metadata": {
    "id": "t9YkGI2J1LuY",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc8bd58a-0474-4a69-bfee-2d4048e1b541",
   "metadata": {
    "id": "bc8bd58a-0474-4a69-bfee-2d4048e1b541"
   },
   "outputs": [],
   "source": [
    "import numpy as np  # Provides extra tools for efficient numerical computations and array manipulation\n",
    "import pandas as pd  # Essential for data manipulation, cleaning, and analysis using DataFrames\n",
    "from sklearn.preprocessing import StandardScaler  # Used to normalize data by scaling features to a standard range\n",
    "import statsmodels.api as sm  # Offers advanced statistical modeling and hypothesis testing capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c692c22f-8c7c-4779-b05c-034dc68eb2f9",
   "metadata": {
    "id": "c692c22f-8c7c-4779-b05c-034dc68eb2f9"
   },
   "source": [
    "### The following funtions enable the relevant feature columns to be scaled, transformed or one-hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z89LUNVx1YI5",
   "metadata": {
    "id": "z89LUNVx1YI5"
   },
   "source": [
    "**Scaling** ‚öñÔ∏è\n",
    "\n",
    "Several scaling methods were tested to optimise model performance:\n",
    "\n",
    "*   Robust Scaling: Scales using quartiles.\n",
    "*   Min-Max Scaling: Scales within a defined range.\n",
    "\n",
    "Ultimately, **Standard Scaling** was chosen as it performed best. It standardizes data using the mean and standard deviation of each column, ensuring uniform scaling across all features in the DataFrame.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e70d7e3-7d29-438c-a2d9-0cac18c41fe9",
   "metadata": {
    "id": "7e70d7e3-7d29-438c-a2d9-0cac18c41fe9"
   },
   "outputs": [],
   "source": [
    "# Define a function which applies standard scaling to the numerical columns:\n",
    "def scaling(df):\n",
    "    df = df.copy()\n",
    "    # List the columns to be scaled\n",
    "    scaled_col_names = [\"Year\", \"Infant_deaths\", \"Under_five_deaths\", \"Adult_mortality\", \"Alcohol_consumption\",\n",
    "                \"Hepatitis_B\", \"Measles\", \"BMI\", \"Polio\", \"Diphtheria\", \"Incidents_HIV\", \"GDP_per_capita\",\n",
    "                \"Population_mln\", \"Thinness_ten_nineteen_years\", \"Thinness_five_nine_years\", \"Schooling\",\n",
    "                \"Economy_status_Developed\", \"Economy_status_Developing\", 'GDP_per_capita_log', 'Incidents_HIV_log']\n",
    "    features = df[scaled_col_names]\n",
    "    # Fit and transform the scaler on the features to be scaled\n",
    "    scaler = StandardScaler().fit(features)\n",
    "    scaled_features = scaler.transform(features)\n",
    "    df[scaled_col_names] = scaled_features\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bUfMxvci5rUc",
   "metadata": {
    "id": "bUfMxvci5rUc"
   },
   "source": [
    "**Log Transformations** ü™µ\n",
    "\n",
    "To meet the assumptions of a linear model, column relationships were checked. **GDP per capita** and **HIV incidents** showed logarithmic relationships with life expectancy, so log transformations were applied to improve model performance.\n",
    "\n",
    "**One-Hot Encoding** üî•\n",
    "\n",
    "Categorical regions were converted into separate columns (one-hot encoded) to enhance model accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f165b4a-7163-496c-a657-8943bdaab427",
   "metadata": {
    "id": "8f165b4a-7163-496c-a657-8943bdaab427"
   },
   "outputs": [],
   "source": [
    "# Define a function applying feature engineering (plus scaling) to the data:\n",
    "def feature_eng(df):\n",
    "        df = df.copy()  # Good practice to use a copy\n",
    "\n",
    "        # Log transformation of the GDP column and the Incidents_HIV column\n",
    "        df['GDP_per_capita_log'] = df['GDP_per_capita'].apply(lambda x: np.log(x))\n",
    "        df['Incidents_HIV_log'] = df['Incidents_HIV'].apply(lambda x: -np.log(x))\n",
    "\n",
    "        # Scale the data using a standard scaler\n",
    "        scaled_df = scaling(df)\n",
    "\n",
    "        # Making region numerical OHE if it's within the dataframe\n",
    "        scaled_df = pd.get_dummies(scaled_df, columns = ['Region'], drop_first = True, prefix = 'Region', dtype = int)\n",
    "\n",
    "        # Add the constant column\n",
    "        scaled_df = sm.add_constant(scaled_df)\n",
    "\n",
    "        # Return the feature engineered result\n",
    "        return scaled_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
